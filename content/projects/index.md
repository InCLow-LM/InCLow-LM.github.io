---
title: Projects
date: 2025-05-24

type: landing

sections:
  - block: markdown
    content:
      title: Current Projects
      text: |-
        **********************************************************************************        
        [Polyglot Machines](https://www.nwo.nl/en/projects/vividi221c009) [2024-]:
        
        This NWO-funded VIDI project aims at improving language modeling for (low-resource) morphologically rich languages, taking inspiration from child language acquisition insights.

        For example, we study the role of different types of language inputs on the morpho-syntactic abilities acquired by the models. Key research questions include: Does training on child-directed language speed up the language learning process compared to training on adult-directed language (such as Wikipedia articles)? Which properties of child-directed language enable efficient learning in humans vs. machines?

        To scale up the research to a broader set of languages, we also develop new grammatical evaluation benchmarks in a cross-lingual or language-specific way, as well as collecting developmentally plausible LM training datasets in non-English languages.

        Key people: Arianna, Jaap, Francesca

        **********************************************************************************

        [LESSEN Project](https://lessen-project.nl/) [2023-]:

        LESSEN is an NWO-funded Netherlands-based consortium bringing together academic and industrial partners working on safe and efficient chat-based AI assistants, with a focus on low-resource (retail) domains. Example partners are Albert Heijn, bol.com or KPN (Dutch telecommunication company), all of which handle large amounts of user requests daily through chatbots.

        Within this consortium, we specifically focus on studying and improving LLMs' abilities to answer user requests consistently in different languages, for instance using Retrieval-Augmented Generation techniques and inspecting model internals to attribute model answers to a specific textual source.

        Key people: Jirui, Arianna, Raquel

        **********************************************************************************

        [InDeep Project](https://projects.illc.uva.nl/indeep/) [2021-]:

        In the InDeep project pioneering researchers in the domain of interpretability of deep learning models of text, language, speech and music are brought together. They collaborate with companies and non-for-profit institutions working with language, speech and music technology, to develop applications that help assess the usefulness of alternative interpretability techniques on a range of different tasks. In “justification” tasks, we look at how interpretability techniques help give users meaningful feedback. Examples include legal and medical document text mining and audio search. In “augmentation” tasks we look at how these techniques facilitate the use of domain knowledge and models from outside deep learning to make the models perform even better. Examples include machine translation, music recommendation and writing feedback. In “interaction” tasks we allow users to influence the functioning of their automated systems, by providing both interpretable information on how the system operates, and letting human produced output find its way into the internal states of the learning algorithm. Examples include adapting speech recognition to non-standard accents and dialects, interactive music generation, and machine assisted translation.

        Key people: Gabriele, Arianna, Malvina, Grzegorz

        **********************************************************************************

        NeLLCom [2019-]:

        This project is the fruit of a long-running collaboration with language evolution expert Tessa Verhoef. Our goal is to use neural network-based agents to simulate and study the emergence of universal language properties, such as the trade-off between word order and case marking as alternative strategies to convey argument roles. For this purpose, we have developed a Neural-agent Language Learning and Communication framework (NeLLCom) that combines supervised learning with reinforcement learning in a meaning reconstruction game. 

        In our experiments, we teach agents small artificial languages that were designed by cognitive scientists for use in experiments with human participants. We then let agents communicate with each other and study how their language changes in comparison to what has been observed in humans.

        To pursue this line of work, we've been fortunate to supervise two PhD students funded by the China Scholarship Council.

        Key people: Arianna, Tessa, Yuchen, Yuqing

        **********************************************************************************
  - block: markdown
    content:
      title: Past Projects
      text: |-
        **********************************************************************************        
        Our journey continues — new projects are on the way!
        **********************************************************************************
        
 
---
