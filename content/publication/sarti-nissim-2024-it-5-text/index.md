---
title: 'IT5: Text-to-text Pretraining for Italian Language Understanding and Generation'
authors:
- Gabriele Sarti
- Malvina Nissim
date: '2024-05-01'
publishDate: '2025-04-30T15:15:13.063334Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)*'
abstract: We introduce IT5, the first family of encoder-decoder transformer models
  pretrained specifically on Italian. We document and perform a thorough cleaning
  procedure for a large Italian corpus and use it to pretrain four IT5 model sizes.
  We then introduce the ItaGen benchmark, which includes a broad range of natural
  language understanding and generation tasks for Italian, and use it to evaluate
  the performance of IT5 models and multilingual baselines. We find monolingual IT5
  models to provide the best scale-to-performance ratio across tested models, consistently
  outperforming their multilingual counterparts and setting a new state-of-the-art
  for Italian language generation.
links:
- name: URL
  url: https://aclanthology.org/2024.lrec-main.823
---
